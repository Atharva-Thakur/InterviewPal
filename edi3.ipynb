{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-generativeai in /home/user4/.local/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.3.3 in /home/user4/.local/lib/python3.10/site-packages (from google-generativeai) (0.3.3)\n",
      "Requirement already satisfied: google-auth in /home/user4/.local/lib/python3.10/site-packages (from google-generativeai) (2.23.4)\n",
      "Requirement already satisfied: google-api-core in /home/user4/.local/lib/python3.10/site-packages (from google-generativeai) (2.14.0)\n",
      "Requirement already satisfied: protobuf in /home/user4/.local/lib/python3.10/site-packages (from google-generativeai) (4.23.4)\n",
      "Requirement already satisfied: tqdm in /home/user4/.local/lib/python3.10/site-packages (from google-generativeai) (4.65.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /home/user4/.local/lib/python3.10/site-packages (from google-ai-generativelanguage==0.3.3->google-generativeai) (1.22.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/user4/.local/lib/python3.10/site-packages (from google-api-core->google-generativeai) (1.61.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /home/user4/.local/lib/python3.10/site-packages (from google-api-core->google-generativeai) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (4.9)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/user4/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.3.3->google-generativeai) (1.59.3)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/user4/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.3.3->google-generativeai) (1.59.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user4/.local/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2020.6.20)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth->google-generativeai) (0.4.8)\n",
      "\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.43ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MLDL-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is padding?</td>\n",
       "      <td>Padding is the addition of extra layers of zer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sigmoid Vs Softmax</td>\n",
       "      <td>Sigmoid is used for binary classification, whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is PoS Tagging?</td>\n",
       "      <td>Part-of-Speech (PoS) tagging is the process of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is tokenization?</td>\n",
       "      <td>Tokenization is the process of breaking down t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is topic modeling?</td>\n",
       "      <td>Topic modeling is a technique in natural langu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question                                             answer\n",
       "0         What is padding?  Padding is the addition of extra layers of zer...\n",
       "1       Sigmoid Vs Softmax  Sigmoid is used for binary classification, whi...\n",
       "2     What is PoS Tagging?  Part-of-Speech (PoS) tagging is the process of...\n",
       "3    What is tokenization?  Tokenization is the process of breaking down t...\n",
       "4  What is topic modeling?  Topic modeling is a technique in natural langu..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm\n",
    "\n",
    "palm.configure(api_key='AIzaSyCp37hYiT7Sjb2SbnCrwSl1D_ppKsrtb-c')\n",
    "\n",
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model = models[0].name\n",
    "\n",
    "convo = \"\"\n",
    "\n",
    "for n in range(0, 5):\n",
    "    i = random.randint(0,len(df)-1)\n",
    "    que = df.iloc[i,0]\n",
    "    ans = df.iloc[i,1]\n",
    "    candidate_ans = input(\"Answer the following question - \"+ que +\" : \")\n",
    "\n",
    "    job_title = \"data scientist\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an interviewer at a big company. \n",
    "    A candidate applying for the position of {job_title} has been asked the question {que}.\n",
    "    The correct answer to the question is {ans}.\n",
    "    The candidate has answered to the given question with {candidate_ans}.\n",
    "    Explain how the candidate has performed in short and kind way.\n",
    "    Also rate the answer out of 10.\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation = palm.generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=0.5,\n",
    "        max_output_tokens=800,\n",
    "    )\n",
    "    eval = evaluation.result\n",
    "    convo = convo + \"Question: \" + que + \"\\nCorrect Answer: \" + ans + \"\\nCandidate Answer: \" + candidate_ans + \"\\nEvaluation: \" + eval + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: What are autoencoders? Explain the different layers of autoencoders and mention three practical usages of them.\\nCorrect Answer: Autoencoders are neural networks designed to encode input data into a lower-dimensional representation. Layers include an encoder, bottleneck layer, and decoder. Usages include data compression, denoising, and feature learning.\\nCandidate Answer: why are u asking me\\nEvaluation: The candidate\\'s answer is not satisfactory. The candidate did not answer the question and instead asked why the interviewer is asking them. This shows that the candidate is not prepared for the interview and does not have a good understanding of autoencoders. I would rate the candidate\\'s answer a 1 out of 10.\\n\\nQuestion: What is the range of activation functions\\nCorrect Answer: Sigmoid ranges from 0 to 1, Tanh from -1 to 1, and ReLU from 0 to positive infinity.\\nCandidate Answer: i dont know and care\\nEvaluation: The candidate\\'s answer is not satisfactory. The correct answer to the question is Sigmoid ranges from 0 to 1, Tanh from -1 to 1, and ReLU from 0 to positive infinity. The candidate\\'s answer \"I don\\'t know and care\" shows that they are not prepared for the interview and do not have a good understanding of the material. I would rate the answer 1 out of 10.\\n\\nQuestion: Do gradient descent methods always converge at the same point?\\nCorrect Answer: No, the convergence point can vary due to factors like the optimization algorithm, learning rate, and the problem\\'s nature.\\nCandidate Answer: why would i know\\nEvaluation: The candidate\\'s answer is not satisfactory. The correct answer to the question is No, the convergence point can vary due to factors like the optimization algorithm, learning rate, and the problem\\'s nature. The candidate\\'s answer does not provide any information about the factors that affect the convergence point, and it is not clear whether the candidate understands the concept of gradient descent. I would rate the answer 1 out of 10.\\n\\nQuestion: Difference between bagging, boosting, and the relation to Bayes\\' theorem\\nCorrect Answer: Bagging and boosting are ensemble learning methods that combine multiple models. Bagging reduces variance, while boosting reduces bias. The relation to Bayes\\' theorem lies in their iterative nature, updating the model based on errors, similar to Bayesian updating.\\nCandidate Answer: mmmmmmmmmmmmmmmmm\\nEvaluation: The candidate\\'s answer is incomprehensible. They did not answer the question and their response is not related to the topic. I rate their answer 0 out of 10.\\n\\nQuestion: How does batch size affect training of neural networks\\nCorrect Answer: Larger batch sizes generally lead to faster convergence but require more memory. Smaller batch sizes introduce noise but can adapt faster to changes in the data.\\nCandidate Answer: who cares\\nEvaluation: The candidate\\'s answer is unacceptable. The correct answer is: Larger batch sizes generally lead to faster convergence but require more memory. Smaller batch sizes introduce noise but can adapt faster to changes in the data.. The candidate\\'s answer is not even relevant to the question. I rate the answer 0 out of 10.\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Question:** You are given a dataset of customer transactions. The goal is to predict the probability that a customer will churn (i.e., stop doing business with the company).\n",
      "\n",
      "What machine learning algorithm would you use to solve this problem? And why?\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an interviewer at a tech company.\n",
    "your job is to ask only ask the question never give any answers.\n",
    "ask the candidate one technical question for the position of {job_title}.\n",
    "\"\"\"\n",
    "\n",
    "res = palm.generate_text(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    temperature=0.5,\n",
    "    max_output_tokens=100,\n",
    ")\n",
    "print(res.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected. The candidate did not answer the questions well and showed a lack of understanding of the material. They could have done better by preparing for the interview more thoroughly and by being more engaged with the interviewer.\n"
     ]
    }
   ],
   "source": [
    "prompt2 = f\"\"\"\n",
    "You are an interviewer at a big company. \n",
    "A candidate applying for the position of {job_title} has given an interview.\n",
    "Analyze the following converstuion of their interview and decide whether the candidate deserves the position or not.\n",
    "conversation : {convo}\n",
    "Answer in one word only - approved or rejected.\n",
    "And in next line explain why you made that decision.\n",
    "And then give suggestions as to what the candidate could have done better.\n",
    "\"\"\"\n",
    "\n",
    "res = palm.generate_text(\n",
    "    model=model,\n",
    "    prompt=prompt2,\n",
    "    temperature=0.5,\n",
    "    max_output_tokens=100,\n",
    ")\n",
    "print(res.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resu():\n",
    "    prompt2 = f\"\"\"\n",
    "    You are an interviewer at a big company. \n",
    "    A candidate applying for the position of {job_title} has given an interview.\n",
    "    Analyze the following converstuion of their interview and decide whether the candidate deserves the position or not.\n",
    "    conversation : {convo}\n",
    "    Answer in one word only - approved or rejected.\n",
    "    And in next line explain why you made that decision.\n",
    "    And then give suggestions as to what the candidate could have done better.\n",
    "    \"\"\"\n",
    "\n",
    "    res = palm.generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt2,\n",
    "        temperature=0.5,\n",
    "        max_output_tokens=100,\n",
    "    )\n",
    "    # print(res.result)\n",
    "    return \"approved\" in res.result\n",
    "\n",
    "resu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
